{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final\n",
    "This notebook has all the 6 submission codes.\n",
    "Preprocessing: Changed 1 and 0 to 1 and -1. Then merged misc with BOW,glove and tfidf using encoding and also fixing the missing values in the age column of misc data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the labels from 0 to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/bag-of-words/bow.train.csv\")\n",
    "df['label'] = df['label'].replace(0, -1)\n",
    "df\n",
    "df.to_csv('new.bow.train.csv', index=False)\n",
    "df=pd.read_csv(\"data/bag-of-words/bow.test.csv\")\n",
    "df['label'] = df['label'].replace(0, -1)\n",
    "df\n",
    "df.to_csv('new.bow.test.csv', index=False)\n",
    "df=pd.read_csv(\"data/glove/glove.train.csv\")\n",
    "df['label'] = df['label'].replace(0, -1)\n",
    "df\n",
    "df.to_csv('new.glove.train.csv', index=False)\n",
    "df=pd.read_csv(\"data/glove/glove.test.csv\")\n",
    "df['label'] = df['label'].replace(0, -1)\n",
    "df\n",
    "df.to_csv('new.glove.test.csv', index=False)\n",
    "df=pd.read_csv(\"data/tfidf/tfidf.test.csv\")\n",
    "df['label'] = df['label'].replace(0, -1)\n",
    "df\n",
    "df.to_csv('new.tfidf.test.csv', index=False)\n",
    "df=pd.read_csv(\"data/tfidf/tfidf.train.csv\")\n",
    "df['label'] = df['label'].replace(0, -1)\n",
    "df\n",
    "df.to_csv('new.tfidf.train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "data1 = pd.read_csv('/home/u1472278/ML/Project/project_data/data/misc/misc-attributes-test.csv')\n",
    "data2 = pd.read_csv('/home/u1472278/ML/Project/project_data/new.tfidf.test.csv')\n",
    "\n",
    "# Handle missing values in 'defendant_age'\n",
    "# Replace 'not known' with NaN and then convert to numeric\n",
    "data1['defendant_age'] = pd.to_numeric(data1['defendant_age'], errors='coerce')\n",
    "median_age = data1['defendant_age'].median()  # Compute median of known values\n",
    "data1['defendant_age'].fillna(median_age, inplace=True)  # Fill missing values with median\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['defendant_gender', 'victim_genders', 'offence_category', 'offence_subcategory']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data1[col] = le.fit_transform(data1[col])\n",
    "    label_encoders[col] = le  # Store the encoders if needed later for inverse transform\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_data = pd.concat([data2.drop('label', axis=1), data1], axis=1)\n",
    "combined_data.insert(0, 'label', data2['label'])  # Inserting the label column at the first position\n",
    "\n",
    "# Convert all columns except 'label' to float\n",
    "for col in combined_data.columns:\n",
    "    if col != 'label':\n",
    "        combined_data[col] = combined_data[col].astype(float)\n",
    " # Inserting the label column at the first position\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_data\n",
    "combined_data.to_csv('tfidf_misc_test.csv', index=False)\n",
    "\n",
    "# print(\"Data has been processed and saved to 'combined_data.csv', with 'label' as the first column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining tfidf and misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "data1 = pd.read_csv('/home/u1472278/ML/Project/project_data/data/misc/misc-attributes-test.csv')\n",
    "data2 = pd.read_csv('/home/u1472278/ML/Project/project_data/new.tfidf.test.csv')\n",
    "\n",
    "# Handle missing values in 'defendant_age'\n",
    "# Replace 'not known' with NaN and then convert to numeric\n",
    "data1['defendant_age'] = pd.to_numeric(data1['defendant_age'], errors='coerce')\n",
    "median_age = data1['defendant_age'].median()  # Compute median of known values\n",
    "data1['defendant_age'].fillna(median_age, inplace=True)  # Fill missing values with median\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['defendant_gender', 'victim_genders', 'offence_category', 'offence_subcategory']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data1[col] = le.fit_transform(data1[col])\n",
    "    label_encoders[col] = le  # Store the encoders if needed later for inverse transform\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_data = pd.concat([data2.drop('label', axis=1), data1], axis=1)\n",
    "combined_data.insert(0, 'label', data2['label'])  # Inserting the label column at the first position\n",
    "\n",
    "# Convert all columns except 'label' to float\n",
    "for col in combined_data.columns:\n",
    "    if col != 'label':\n",
    "        combined_data[col] = combined_data[col].astype(float)\n",
    " # Inserting the label column at the first position\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_data\n",
    "combined_data.to_csv('tfidf_misc_test.csv', index=False)\n",
    "\n",
    "# print(\"Data has been processed and saved to 'combined_data.csv', with 'label' as the first column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "data1 = pd.read_csv('/home/u1472278/ML/Project/project_data/data/misc/misc-attributes-train.csv')\n",
    "data2 = pd.read_csv('/home/u1472278/ML/Project/project_data/new.tfidf.train.csv')\n",
    "\n",
    "# Handle missing values in 'defendant_age'\n",
    "# Replace 'not known' with NaN and then convert to numeric\n",
    "data1['defendant_age'] = pd.to_numeric(data1['defendant_age'], errors='coerce')\n",
    "median_age = data1['defendant_age'].median()  # Compute median of known values\n",
    "data1['defendant_age'].fillna(median_age, inplace=True)  # Fill missing values with median\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['defendant_gender', 'victim_genders', 'offence_category', 'offence_subcategory']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data1[col] = le.fit_transform(data1[col])\n",
    "    label_encoders[col] = le  # Store the encoders if needed later for inverse transform\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_data = pd.concat([data2.drop('label', axis=1), data1], axis=1)\n",
    "combined_data.insert(0, 'label', data2['label'])  # Inserting the label column at the first position\n",
    "\n",
    "# Convert all columns except 'label' to float\n",
    "for col in combined_data.columns:\n",
    "    if col != 'label':\n",
    "        combined_data[col] = combined_data[col].astype(float)\n",
    " # Inserting the label column at the first position\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_data\n",
    "combined_data.to_csv('tfidf_misc_train.csv', index=False)\n",
    "\n",
    "# print(\"Data has been processed and saved to 'combined_data.csv', with 'label' as the first column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "data1 = pd.read_csv('/home/u1472278/ML/Project/project_data/data/misc/misc-attributes-eval.csv')\n",
    "data2 = pd.read_csv('/home/u1472278/ML/Project/project_data/data/tfidf/tfidf.eval.anon.csv')\n",
    "\n",
    "# Handle missing values in 'defendant_age'\n",
    "# Replace 'not known' with NaN and then convert to numeric\n",
    "data1['defendant_age'] = pd.to_numeric(data1['defendant_age'], errors='coerce')\n",
    "median_age = data1['defendant_age'].median()  # Compute median of known values\n",
    "data1['defendant_age'].fillna(median_age, inplace=True)  # Fill missing values with median\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['defendant_gender', 'victim_genders', 'offence_category', 'offence_subcategory']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data1[col] = le.fit_transform(data1[col])\n",
    "    label_encoders[col] = le  # Store the encoders if needed later for inverse transform\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_data = pd.concat([data2.drop('label', axis=1), data1], axis=1)\n",
    "combined_data.insert(0, 'label', data2['label'])  # Inserting the label column at the first position\n",
    "\n",
    "# Convert all columns except 'label' to float\n",
    "for col in combined_data.columns:\n",
    "    if col != 'label':\n",
    "        combined_data[col] = combined_data[col].astype(float)\n",
    " # Inserting the label column at the first position\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_data\n",
    "combined_data.to_csv('tfidf_misc_eval.csv', index=False)\n",
    "\n",
    "# print(\"Data has been processed and saved to 'combined_data.csv', with 'label' as the first column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bow+misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We change this code for training and testin data too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "data2 = pd.read_csv('/home/u1472278/ML/Project/project_data/data/bag-of-words/bow.eval.anon.csv')\n",
    "data1 = pd.read_csv('/home/u1472278/ML/Project/project_data/data/misc/misc-attributes-eval.csv')\n",
    "\n",
    "# Handle missing values in 'defendant_age'\n",
    "# Replace 'not known' with NaN and then convert to numeric\n",
    "data1['defendant_age'] = pd.to_numeric(data1['defendant_age'], errors='coerce')\n",
    "median_age = data1['defendant_age'].median()  # Compute median of known values\n",
    "data1['defendant_age'].fillna(median_age, inplace=True)  # Fill missing values with median\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['defendant_gender', 'victim_genders', 'offence_category', 'offence_subcategory']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data1[col] = le.fit_transform(data1[col])\n",
    "    label_encoders[col] = le  # Store the encoders if needed later for inverse transform\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_data = pd.concat([data2.drop('label', axis=1), data1], axis=1)\n",
    "combined_data.insert(0, 'label', data2['label'])  # Inserting the label column at the first position\n",
    "\n",
    "# Convert all columns except 'label' to float\n",
    "for col in combined_data.columns:\n",
    "    if col != 'label':\n",
    "        combined_data[col] = combined_data[col].astype(float)\n",
    " # Inserting the label column at the first position\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_data\n",
    "combined_data.to_csv('bow_misc_eval.csv', index=False)\n",
    "\n",
    "# print(\"Data has been processed and saved to 'combined_data.csv', with 'label' as the first column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove+ misc\n",
    "# Again change it for testing and training both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "data1 = pd.read_csv('/home/u1472278/ML/Project/project_data/data/misc/misc-attributes-eval.csv')\n",
    "data2 = pd.read_csv('/home/u1472278/ML/Project/project_data/data/glove/glove.eval.anon.csv')\n",
    "\n",
    "# Handle missing values in 'defendant_age'\n",
    "# Replace 'not known' with NaN and then convert to numeric\n",
    "data1['defendant_age'] = pd.to_numeric(data1['defendant_age'], errors='coerce')\n",
    "median_age = data1['defendant_age'].median()  # Compute median of known values\n",
    "data1['defendant_age'].fillna(median_age, inplace=True)  # Fill missing values with median\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['defendant_gender', 'victim_genders', 'offence_category', 'offence_subcategory']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data1[col] = le.fit_transform(data1[col])\n",
    "    label_encoders[col] = le  # Store the encoders if needed later for inverse transform\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_data = pd.concat([data2.drop('label', axis=1), data1], axis=1)\n",
    "combined_data.insert(0, 'label', data2['label'])  # Inserting the label column at the first position\n",
    "\n",
    "# Convert all columns except 'label' to float\n",
    "for col in combined_data.columns:\n",
    "    if col != 'label':\n",
    "        combined_data[col] = combined_data[col].astype(float)\n",
    " # Inserting the label column at the first position\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_data\n",
    "combined_data.to_csv('glove_misc_eval.csv', index=False)\n",
    "\n",
    "# print(\"Data has been processed and saved to 'combined_data.csv', with 'label' as the first column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal perceptron with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.total_updates = 0\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialize weights and bias\n",
    "        self.w = np.random.uniform(-0.01, 0.01, size=n_features)\n",
    "        self.b = np.random.uniform(-0.01, 0.01)\n",
    "\n",
    "        # Training loop\n",
    "        for _ in range(epochs):\n",
    "            for i in range(n_samples):\n",
    "                if y[i] * (np.dot(X[i], self.w) + self.b) <= 0:\n",
    "                    # Update weights and bias\n",
    "                    self.w += self.learning_rate * y[i] * X[i]\n",
    "                    self.b += self.learning_rate * y[i]\n",
    "                    self.total_updates += 1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "def split_data(X_train, y_train, fold, num_folds):\n",
    "    fold_size = len(X_train) // num_folds\n",
    "    start = fold * fold_size\n",
    "    end = (fold + 1) * fold_size\n",
    "\n",
    "    X_val_fold = X_train[start:end]\n",
    "    y_val_fold = y_train[start:end]\n",
    "\n",
    "    X_train_fold = np.concatenate([X_train[:start], X_train[end:]])\n",
    "    y_train_fold = np.concatenate([y_train[:start], y_train[end:]])\n",
    "\n",
    "    return X_train_fold, y_train_fold, X_val_fold, y_val_fold\n",
    "\n",
    "def cross_validation(learning_rates, epochs_cv, X_train, y_train):\n",
    "    results = []\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        fold_accuracies = []\n",
    "\n",
    "        for _ in range(epochs_cv):\n",
    "            fold_accuracy = 0\n",
    "\n",
    "            for fold in range(5):\n",
    "                X_train_fold, y_train_fold, X_val_fold, y_val_fold = split_data(X_train, y_train, fold, 5)\n",
    "\n",
    "                perceptron = Perceptron(eta)\n",
    "                perceptron.train(X_train_fold, y_train_fold, epochs=1)\n",
    "                fold_accuracy += evaluate(X_val_fold, y_val_fold, perceptron)\n",
    "\n",
    "            fold_accuracy /= 5  # Average accuracy across folds\n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "        avg_accuracy = np.mean(fold_accuracies)\n",
    "        results.append((eta, avg_accuracy))\n",
    "\n",
    "    best_eta, best_accuracy = max(results, key=lambda x: x[1])\n",
    "    return best_eta, best_accuracy\n",
    "\n",
    "def evaluate(X, y, perceptron):\n",
    "    y_pred = perceptron.predict(X)\n",
    "    accuracy = np.mean(y_pred == y)\n",
    "    return accuracy\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"new.bow.train.csv\", header=None, skiprows=1)\n",
    "X_train = train_data.iloc[:, 1:].values.astype(float)\n",
    "y_train = train_data.iloc[:, 0].values.astype(int)\n",
    "\n",
    "dev_data = pd.read_csv(\"data/bag-of-words/bow.eval.anon.csv\", header=None, skiprows=1)\n",
    "X_dev = dev_data.iloc[:, 1:].values.astype(float)\n",
    "y_dev = dev_data.iloc[:, 0].values.astype(int)\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rates = [1, 0.1, 0.01]\n",
    "epochs_cv = 10\n",
    "\n",
    "# Cross-validation to find the best learning rate\n",
    "best_eta, best_accuracy = cross_validation(learning_rates, epochs_cv, X_train, y_train)\n",
    "print(\"Best learning rate found:\", best_eta)\n",
    "print(\"Best accuracy: \", best_accuracy)\n",
    "\n",
    "# Train the model with the best learning rate for more epochs\n",
    "epochs_train = 20\n",
    "perceptron = Perceptron(best_eta)\n",
    "best_perceptron = perceptron.train(X_train, y_train, epochs_train)\n",
    "print(\"Total number of updates on the training set:\", perceptron.total_updates)\n",
    "\n",
    "# Evaluate the best-performing perceptron on the test set\n",
    "test_data = pd.read_csv(\"/home/u1472278/ML/Project/project_data/new.bow.test.csv\", header=None, skiprows=1)\n",
    "X_test = test_data.iloc[:, 1:].values.astype(float)\n",
    "y_test = test_data.iloc[:, 0].values.astype(int)\n",
    "test_accuracy = evaluate(X_test, y_test, best_perceptron)\n",
    "print(\"Test accuracy using the best-performing perceptron:\", test_accuracy)\n",
    "\n",
    "# Evaluate the best-performing perceptron on the dev set\n",
    "# dev_accuracy = evaluate(X_dev, y_dev, perceptron)\n",
    "# print(\"Dev accuracy:\", dev_accuracy)\n",
    "\n",
    "# Predict labels for the dev set and save to submission file\n",
    "predicted_labels = best_perceptron.predict(X_dev)\n",
    "predicted_labels_int = predicted_labels.astype(int)\n",
    "\n",
    "predicted_labels_df = pd.DataFrame(predicted_labels_int, columns=['label'])\n",
    "eval_ids = pd.read_csv(\"data/eval.ids\", header=None, names=['example_id'])\n",
    "\n",
    "submission_df = pd.concat([eval_ids['example_id'], predicted_labels_df], axis=1)\n",
    "\n",
    "# Save the concatenated DataFrame to a CSV file\n",
    "#submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced perceptron with bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"new.bow.train.csv\", header=None, skiprows=1)\n",
    "X_train = train_data.iloc[:, 1:].values.astype(float)\n",
    "y_train = train_data.iloc[:, 0].values.astype(int)\n",
    "\n",
    "# Normalize and scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train Perceptron model\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Evaluate model on validation set\n",
    "y_pred_val = perceptron.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# Train model on full training set\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv(\"new.bow.test.csv\", header=None, skiprows=1)\n",
    "X_test = test_data.iloc[:, 1:].values.astype(float)\n",
    "X_test_scaled = scaler.transform(X_test)  # Scale test data using the same scaler\n",
    "\n",
    "# Predict labels for test set\n",
    "y_pred_test = perceptron.predict(X_test_scaled)\n",
    "\n",
    "# Save predictions to submission file\n",
    "eval_data = pd.read_csv(\"data/bag-of-words/bow.eval.anon.csv\", header=None, skiprows=1)\n",
    "X_eval = eval_data.iloc[:, 1:].values.astype(float)\n",
    "X_eval_scaled = scaler.transform(X_eval)  # Scale evaluation data using the same scaler\n",
    "\n",
    "# Predict labels for evaluation set\n",
    "y_pred_eval = perceptron.predict(X_eval_scaled)\n",
    "\n",
    "predicted_labels_int = y_pred_eval.astype(int)\n",
    "\n",
    "predicted_labels_df = pd.DataFrame(predicted_labels_int, columns=['label'])\n",
    "eval_ids = pd.read_csv(\"data/eval.ids\", header=None, names=['example_id'])\n",
    "\n",
    "submission_df = pd.concat([eval_ids['example_id'], predicted_labels_df], axis=1)\n",
    "\n",
    "# Save the concatenated DataFrame to a CSV file\n",
    "submission_df.to_csv('solution.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with tfidf+misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# def compute_gradient(X, y, w, C):\n",
    "#     m = X.shape[0]\n",
    "#     y_pred = sigmoid(np.dot(X, w)) * 2 - 1  # Transform sigmoid output to -1 and +1\n",
    "#     gradient = np.dot(X.T, (y_pred - y)) / m + C * w\n",
    "#     return gradient\n",
    "def compute_svm_gradient(X, y, w, C):\n",
    "    m = X.shape[0]\n",
    "    distances = 1 - y * (np.dot(X, w))\n",
    "    dw = np.zeros(len(w))\n",
    "    for ind, d in enumerate(distances):\n",
    "        if max(0, d) == 0:\n",
    "            di = w\n",
    "        else:\n",
    "            di = w - (C * y[ind] * X[ind])\n",
    "        dw += di\n",
    "    dw = dw/m  # Average\n",
    "    return dw\n",
    "def logistic_regression_SGD(X_train, y_train, learning_rate, epochs, C, batch_size=32):\n",
    "    w = np.zeros(X_train.shape[1])\n",
    "    num_samples = X_train.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "            gradient = compute_svm_gradient(X_batch, y_batch, w, C)\n",
    "            w -= learning_rate * gradient\n",
    "\n",
    "        learning_rate /= (1 + epoch)  # Decaying learning rate\n",
    "\n",
    "    return w\n",
    "\n",
    "def predict(X, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    return np.where(sigmoid(z) >= 0.5, 1, -1)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    false_positives = np.sum((y_pred == 1) & (y_true == -1))\n",
    "    false_negatives = np.sum((y_pred == -1) & (y_true == 1))\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    total_predictions = len(y_true)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def k_fold_split(num_samples, k=5):\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    fold_sizes = np.full(k, num_samples // k, dtype=int)\n",
    "    fold_sizes[:num_samples % k] += 1\n",
    "    current = 0\n",
    "    folds = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        folds.append(indices[start:stop])\n",
    "        current = stop\n",
    "    return folds\n",
    "\n",
    "def perform_cross_validation(X, y, k, learning_rates, Cs, epochs, batch_size):\n",
    "    num_samples = len(y)\n",
    "    folds = k_fold_split(num_samples, k)\n",
    "    best_params = {}\n",
    "    best_f1 = -np.inf\n",
    "\n",
    "    for gamma in learning_rates:\n",
    "        for C in Cs:\n",
    "            f1_scores = []\n",
    "\n",
    "            for i in range(k):\n",
    "                test_idx = folds[i]\n",
    "                train_idx = np.hstack([folds[j] for j in range(k) if j != i])\n",
    "\n",
    "                X_train, X_valid = X[train_idx], X[test_idx]\n",
    "                y_train, y_valid = y[train_idx], y[test_idx]\n",
    "                \n",
    "                weights = logistic_regression_SGD(X_train, y_train, gamma, epochs, C, batch_size)\n",
    "                y_pred = predict(X_valid, weights)\n",
    "                \n",
    "                _, _, f1 = calculate_metrics(y_valid, y_pred)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "            avg_f1 = np.mean(f1_scores)\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                best_params = {'gamma': gamma, 'C': C}\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('/home/u1472278/ML/Project/project_data/tfidf_misc_train.csv')\n",
    "dev_data = pd.read_csv('/home/u1472278/ML/Project/project_data/tfidf_misc_eval.csv')\n",
    "test_data = pd.read_csv('/home/u1472278/ML/Project/project_data/tfidf_misc_test.csv')\n",
    "\n",
    "X_train = train_data.iloc[:, 1:].values\n",
    "y_train = train_data.iloc[:, 0].values\n",
    "X_dev = dev_data.iloc[:, 1:].values\n",
    "y_dev = dev_data.iloc[:, 0].values\n",
    "X_test = test_data.iloc[:, 1:].values\n",
    "y_test = test_data.iloc[:, 0].values\n",
    "\n",
    "# Hyperparameters and training configuration\n",
    "learning_rates = [0.1, 0.01, 0.001]  # Simplified hyperparameter space\n",
    "Cs = [0.01, 0.1, 1]\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "k = 5  # Number of folds\n",
    "\n",
    "# Perform cross-validation to find the best hyperparameters\n",
    "best_hyperparameters = perform_cross_validation(X_train, y_train, k, learning_rates, Cs, epochs, batch_size)\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "\n",
    "# Train final model on the full training data\n",
    "final_weights = logistic_regression_SGD(X_train, y_train, best_hyperparameters['gamma'], epochs, best_hyperparameters['C'], batch_size)\n",
    "\n",
    "# Predict and evaluate on the dev set\n",
    "y_pred_dev = predict(X_dev, final_weights)\n",
    "dev_precision, dev_recall, dev_f1 = calculate_metrics(y_dev, y_pred_dev)\n",
    "dev_accuracy = calculate_accuracy(y_dev, y_pred_dev)\n",
    "\n",
    "predicted_labels_int = y_pred_dev.astype(int)\n",
    "predicted_labels_df = pd.DataFrame(predicted_labels_int, columns=['label'])\n",
    "eval_ids = pd.read_csv(\"data/eval.ids\", header=None, names=['example_id'])\n",
    "\n",
    "submission_df = pd.concat([eval_ids['example_id'], predicted_labels_df], axis=1)\n",
    "\n",
    "# Save the concatenated DataFrame to a CSV file\n",
    "submission_df.to_csv('svm_prediction_tfidfmisc.csv', index=False)\n",
    "\n",
    "\n",
    "print(f\"Dev Precision: {dev_precision:.4f}\")\n",
    "print(f\"Dev Recall: {dev_recall:.4f}\")\n",
    "print(f\"Dev F1 Score: {dev_f1:.4f}\")\n",
    "print(f\"Dev Accuracy: {dev_accuracy:.4f}\")\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_pred_test = predict(X_test, final_weights)\n",
    "test_precision, test_recall, test_f1 = calculate_metrics(y_test, y_pred_test)\n",
    "test_accuracy = calculate_accuracy(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR with tfidf+misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)  # Clip to prevent overflow in the exponential calculation\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_gradient(X, y, w, sigma2):\n",
    "    m = X.shape[0]\n",
    "    y_pred = sigmoid(np.dot(X, w)) * 2 - 1  # Transform sigmoid output to -1 and +1\n",
    "    gradient = np.dot(X.T, (y_pred - y)) / m + w / sigma2  # L2 regularization\n",
    "    return gradient\n",
    "\n",
    "def logistic_regression_SGD(X_train, y_train, learning_rate, epochs, sigma2, batch_size=32):\n",
    "    w = np.zeros(X_train.shape[1])  # Initialize weights to zeros\n",
    "    num_samples = X_train.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            X_batch = X_train_shuffled[start:end]\n",
    "            y_batch = y_train_shuffled[start:end]\n",
    "            \n",
    "            gradient = compute_gradient(X_batch, y_batch, w, sigma2)\n",
    "            w -= learning_rate * gradient\n",
    "\n",
    "        learning_rate /= (1 + epoch)  # Decaying learning rate each epoch\n",
    "\n",
    "    return w\n",
    "\n",
    "def predict(X, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    return np.where(sigmoid(z) >= 0.5, 1, -1)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    false_positives = np.sum((y_pred == 1) & (y_true == -1))\n",
    "    false_negatives = np.sum((y_pred == -1) & (y_true == 1))\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    total_predictions = len(y_true)\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "def perform_cross_validation(X, y, k, learning_rates, sigma2_values, epochs, batch_size):\n",
    "    num_samples = len(y)\n",
    "    folds = k_fold_split(num_samples, k)\n",
    "    best_f1 = -np.inf\n",
    "    best_params = {}\n",
    "\n",
    "    for gamma in learning_rates:\n",
    "        for sigma2 in sigma2_values:\n",
    "            f1_scores = []\n",
    "\n",
    "            for i in range(k):\n",
    "                test_idx = folds[i]\n",
    "                train_idx = np.hstack([folds[j] for j in range(k) if j != i])\n",
    "\n",
    "                X_train, X_valid = X[train_idx], X[test_idx]\n",
    "                y_train, y_valid = y[train_idx], y[test_idx]\n",
    "                \n",
    "                weights = logistic_regression_SGD(X_train, y_train, gamma, epochs, sigma2, batch_size)\n",
    "                y_pred_valid = predict(X_valid, weights)\n",
    "                _, _, f1 = calculate_metrics(y_valid, y_pred_valid)\n",
    "                \n",
    "                f1_scores.append(f1)\n",
    "\n",
    "            avg_f1 = np.mean(f1_scores)\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                best_params = {'gamma': gamma, 'sigma2': sigma2}\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def k_fold_split(num_samples, k=5):\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    fold_sizes = np.full(k, num_samples // k, dtype=int)\n",
    "    fold_sizes[:num_samples % k] += 1\n",
    "    current = 0\n",
    "    folds = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        folds.append(indices[start:stop])\n",
    "        current = stop\n",
    "    return folds\n",
    "\n",
    "# Load datasets\n",
    "full_train_data = pd.read_csv('/home/u1472278/ML/Project/project_data/tfidf_misc_train.csv')\n",
    "dev_data = pd.read_csv('/home/u1472278/ML/Project/project_data/tfidf_misc_eval.csv')\n",
    "test_data = pd.read_csv('/home/u1472278/ML/Project/project_data/tfidf_misc_test.csv')\n",
    "\n",
    "X_train_full = full_train_data.iloc[:, 1:].values\n",
    "y_train_full = full_train_data.iloc[:, 0].values\n",
    "X_dev = dev_data.iloc[:, 1:].values\n",
    "y_dev = dev_data.iloc[:, 0].values\n",
    "X_test = test_data.iloc[:, 1:].values\n",
    "y_test = test_data.iloc[:, 0].values\n",
    "\n",
    "# Hyperparameters and training configuration\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "sigma2_values = [0.01, 0.1, 1]\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "k = 5  # Number of folds\n",
    "\n",
    "# Perform cross-validation to find the best hyperparameters\n",
    "best_params = perform_cross_validation(X_train_full, y_train_full, k, learning_rates, sigma2_values, epochs, batch_size)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train on the full training data using the best hyperparameters\n",
    "final_weights = logistic_regression_SGD(X_train_full, y_train_full, best_params['gamma'], epochs, best_params['sigma2'], batch_size)\n",
    "\n",
    "# Predict and evaluate on the dev set\n",
    "y_pred_dev = predict(X_dev, final_weights)\n",
    "dev_precision, dev_recall, dev_f1 = calculate_metrics(y_dev, y_pred_dev)\n",
    "dev_accuracy = calculate_accuracy(y_dev, y_pred_dev)\n",
    "\n",
    "predicted_labels_int = y_pred_dev.astype(int)\n",
    "predicted_labels_df = pd.DataFrame(predicted_labels_int, columns=['label'])\n",
    "eval_ids = pd.read_csv(\"data/eval.ids\", header=None, names=['example_id'])\n",
    "\n",
    "submission_df = pd.concat([eval_ids['example_id'], predicted_labels_df], axis=1)\n",
    "\n",
    "# Save the concatenated DataFrame to a CSV file\n",
    "submission_df.to_csv('logistic_regression_prediction_tfidf_misc.csv', index=False)\n",
    "print(f\"Dev Precision: {dev_precision:.4f}\")\n",
    "print(f\"Dev Recall: {dev_recall:.4f}\")\n",
    "print(f\"Dev F1 Score: {dev_f1:.4f}\")\n",
    "print(f\"Dev Accuracy: {dev_accuracy:.4f}\")\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_pred_test = predict(X_test, final_weights)\n",
    "test_precision, test_recall, test_f1 = calculate_metrics(y_test, y_pred_test)\n",
    "test_accuracy = calculate_accuracy(y_test, y_pred_test)\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple perceptron with glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.total_updates = 0\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialize weights and bias\n",
    "        self.w = np.random.uniform(-0.01, 0.01, size=n_features)\n",
    "        self.b = np.random.uniform(-0.01, 0.01)\n",
    "\n",
    "        # Training loop\n",
    "        for _ in range(epochs):\n",
    "            for i in range(n_samples):\n",
    "                if y[i] * (np.dot(X[i], self.w) + self.b) <= 0:\n",
    "                    # Update weights and bias\n",
    "                    self.w += self.learning_rate * y[i] * X[i]\n",
    "                    self.b += self.learning_rate * y[i]\n",
    "                    self.total_updates += 1\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "def split_data(X_train, y_train, fold, num_folds):\n",
    "    fold_size = len(X_train) // num_folds\n",
    "    start = fold * fold_size\n",
    "    end = (fold + 1) * fold_size\n",
    "\n",
    "    X_val_fold = X_train[start:end]\n",
    "    y_val_fold = y_train[start:end]\n",
    "\n",
    "    X_train_fold = np.concatenate([X_train[:start], X_train[end:]])\n",
    "    y_train_fold = np.concatenate([y_train[:start], y_train[end:]])\n",
    "\n",
    "    return X_train_fold, y_train_fold, X_val_fold, y_val_fold\n",
    "\n",
    "def cross_validation(learning_rates, epochs_cv, X_train, y_train):\n",
    "    results = []\n",
    "\n",
    "    for eta in learning_rates:\n",
    "        fold_accuracies = []\n",
    "\n",
    "        for _ in range(epochs_cv):\n",
    "            fold_accuracy = 0\n",
    "\n",
    "            for fold in range(5):\n",
    "                X_train_fold, y_train_fold, X_val_fold, y_val_fold = split_data(X_train, y_train, fold, 5)\n",
    "\n",
    "                perceptron = Perceptron(eta)\n",
    "                perceptron.train(X_train_fold, y_train_fold, epochs=1)\n",
    "                fold_accuracy += evaluate(X_val_fold, y_val_fold, perceptron)\n",
    "\n",
    "            fold_accuracy /= 5  # Average accuracy across folds\n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "        avg_accuracy = np.mean(fold_accuracies)\n",
    "        results.append((eta, avg_accuracy))\n",
    "\n",
    "    best_eta, best_accuracy = max(results, key=lambda x: x[1])\n",
    "    return best_eta, best_accuracy\n",
    "\n",
    "def evaluate(X, y, perceptron):\n",
    "    y_pred = perceptron.predict(X)\n",
    "    accuracy = np.mean(y_pred == y)\n",
    "    return accuracy\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"new.glove.train.csv\", header=None, skiprows=1)\n",
    "X_train = train_data.iloc[:, 1:].values.astype(float)\n",
    "y_train = train_data.iloc[:, 0].values.astype(int)\n",
    "\n",
    "dev_data = pd.read_csv(\"data/glove/glove.eval.anon.csv\", header=None, skiprows=1)\n",
    "X_dev = dev_data.iloc[:, 1:].values.astype(float)\n",
    "y_dev = dev_data.iloc[:, 0].values.astype(int)\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rates = [1, 0.1, 0.01]\n",
    "epochs_cv = 10\n",
    "\n",
    "# Cross-validation to find the best learning rate\n",
    "best_eta, best_accuracy = cross_validation(learning_rates, epochs_cv, X_train, y_train)\n",
    "print(\"Best learning rate found:\", best_eta)\n",
    "print(\"Best accuracy: \", best_accuracy)\n",
    "\n",
    "# Train the model with the best learning rate for more epochs\n",
    "epochs_train = 20\n",
    "perceptron = Perceptron(best_eta)\n",
    "best_perceptron = perceptron.train(X_train, y_train, epochs_train)\n",
    "print(\"Total number of updates on the training set:\", perceptron.total_updates)\n",
    "\n",
    "# Evaluate the best-performing perceptron on the test set\n",
    "test_data = pd.read_csv(\"new.glove.test.csv\", header=None, skiprows=1)\n",
    "X_test = test_data.iloc[:, 1:].values.astype(float)\n",
    "y_test = test_data.iloc[:, 0].values.astype(int)\n",
    "test_accuracy = evaluate(X_test, y_test, best_perceptron)\n",
    "print(\"Test accuracy using the best-performing perceptron:\", test_accuracy)\n",
    "\n",
    "# Evaluate the best-performing perceptron on the dev set\n",
    "# dev_accuracy = evaluate(X_dev, y_dev, perceptron)\n",
    "# print(\"Dev accuracy:\", dev_accuracy)\n",
    "\n",
    "# Predict labels for the dev set and save to submission file\n",
    "predicted_labels = best_perceptron.predict(X_dev)\n",
    "predicted_labels_int = predicted_labels.astype(int)\n",
    "\n",
    "predicted_labels_df = pd.DataFrame(predicted_labels_int, columns=['label'])\n",
    "predicted_labels_df\n",
    "eval_ids = pd.read_csv(\"data/eval.ids\", header=None, names=['example_id'])\n",
    "\n",
    "submission_df = pd.concat([eval_ids['example_id'], predicted_labels_df], axis=1)\n",
    "\n",
    "# Save the concatenated DataFrame to a CSV file\n",
    "submission_df.to_csv('submission_glove.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adaboost with perceptron on tfidf+misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Implementing the Perceptron and AdaBoostPerceptron classes\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def train(self, X, y, epochs=1, sample_weights=None):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.random.uniform(-0.01, 0.01, size=n_features)\n",
    "        self.b = np.random.uniform(-0.01, 0.01)\n",
    "        \n",
    "        if sample_weights is None:\n",
    "            sample_weights = np.ones(n_samples)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            for i in range(n_samples):\n",
    "                if y[i] * (np.dot(X[i], self.w) + self.b) <= 0:\n",
    "                    update = self.learning_rate * sample_weights[i] * y[i]\n",
    "                    self.w += update * X[i]\n",
    "                    self.b += update\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "class AdaBoostPerceptron:\n",
    "    def __init__(self, n_clf=10):\n",
    "        self.n_clf = n_clf\n",
    "        self.clfs = []\n",
    "        self.clf_weights = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples = len(y)\n",
    "        D = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        for _ in range(self.n_clf):\n",
    "            clf = Perceptron(learning_rate=1.0)\n",
    "            clf.train(X, y, epochs=1, sample_weights=D)\n",
    "            pred = clf.predict(X)\n",
    "            error = np.dot(D, (pred != y))\n",
    "            \n",
    "            if error == 0:\n",
    "                self.clfs.append(clf)\n",
    "                self.clf_weights.append(1)\n",
    "                break\n",
    "            \n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            self.clfs.append(clf)\n",
    "            self.clf_weights.append(alpha)\n",
    "            \n",
    "            D *= np.exp(-alpha * y * pred)\n",
    "            D /= D.sum()\n",
    "\n",
    "    def predict(self, X):\n",
    "        clf_preds = np.array([clf.predict(X) for clf in self.clfs])\n",
    "        final_pred = np.dot(self.clf_weights, clf_preds)\n",
    "        return np.sign(final_pred)\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(\"/home/u1472278/ML/Project/project_data/tfidf_misc_train.csv\")\n",
    "X_train = train_data.iloc[:, 1:].values\n",
    "y_train = train_data.iloc[:, 0].values\n",
    "\n",
    "test_data = pd.read_csv(\"/home/u1472278/ML/Project/project_data/tfidf_misc_test.csv\")\n",
    "X_test = test_data.iloc[:, 1:].values\n",
    "y_test = test_data.iloc[:, 0].values\n",
    "\n",
    "dev_data = pd.read_csv(\"/home/u1472278/ML/Project/project_data/tfidf_misc_eval.csv\")\n",
    "X_dev = dev_data.iloc[:, 1:].values\n",
    "\n",
    "# Train AdaBoost Perceptron\n",
    "adaboost_perceptron = AdaBoostPerceptron(n_clf=10)\n",
    "adaboost_perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "y_pred_test = adaboost_perceptron.predict(X_test)\n",
    "test_accuracy = np.mean(y_pred_test == y_test)\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "\n",
    "# Predict labels for the development set\n",
    "y_pred_dev = adaboost_perceptron.predict(X_dev)\n",
    "predicted_labels_int = y_pred_dev.astype(int)\n",
    "\n",
    "predicted_labels_df = pd.DataFrame(predicted_labels_int, columns=['label'])\n",
    "eval_ids = pd.read_csv(\"data/eval.ids\", header=None, names=['example_id'])\n",
    "\n",
    "submission_df = pd.concat([eval_ids['example_id'], predicted_labels_df], axis=1)\n",
    "\n",
    "# Save the concatenated DataFrame to a CSV file\n",
    "submission_df.to_csv('submission_adaboost_tfidf_misc.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
